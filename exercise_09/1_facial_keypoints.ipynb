{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jvuNZQOgCPTc"
   },
   "source": [
    "# Facial Keypoint Detection\n",
    "\n",
    "Welcome back for exercise 9! As we told you, the exercises of this lecture can be subdivided into mainly two parts. The first part in which we re-invented the wheel and implemented the most important methods on our own and the second part, where we start using existing libraries (that already have implemented all the methods). It's now time to start playing around with more complex network architectures. \n",
    "\n",
    "We've already entered stage two, but with the introduction of Convolutional Neural Networks this week, we are given a very powerful tool that we want to explore in this exercises. Therefore, in this week's exercise your task is to build a Convolutional Neural Network to perform facial keypoint detection. \n",
    "\n",
    "Before we start, let's take a look at some example images and corresponding facial keypoints:\n",
    "\n",
    "<img src='images/key_pts_example.png' width=70% height=70%/>\n",
    "\n",
    "The facial keypoints (also called facial landmarks) are the small magenta dots shown on each of the faces in the images above. These keypoints mark important areas of the face: the eyes, corners of the mouth, the nose, etc. and are relevant for a variety of computer vision tasks, such as face filters, emotion recognition, pose recognition, and more. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PYvPUcpPCPTh"
   },
   "source": [
    "## (Optional) Mount folder in Colab\n",
    "\n",
    "Uncomment thefollowing cell to mount your gdrive if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27848,
     "status": "ok",
     "timestamp": 1656687919453,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "XThLiWNqCPTi",
    "outputId": "b75330d6-9590-43d7-e46b-bf5919e4ca97"
   },
   "outputs": [],
   "source": [
    "# Use the following lines if you want to use Google Colab\n",
    "# We presume you created a folder \"i2dl\" within your main drive folder, and put the exercise there.\n",
    "# NOTE: terminate all other colab sessions that use GPU!\n",
    "# NOTE 2: Make sure the correct exercise folder (e.g exercise_09) is given.\n",
    "\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_09'\n",
    "\n",
    "# This will mount your google drive under 'MyDrive'\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "# In order to access the files in this notebook we have to navigate to the correct folder\n",
    "os.chdir(gdrive_path)\n",
    "\n",
    "# Check manually if all files are present\n",
    "print(sorted(os.listdir()))\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up PyTorch environment in colab\n",
    "- (OPTIONAL) Enable GPU via Runtime --> Change runtime type --> GPU\n",
    "- Uncomment the following cell if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install correct libraries in google colab\n",
    "# !python -m pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchtext==0.12.0+cu113 torchaudio==0.12.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !python -m pip install tensorboard==2.8.0 > /dev/null"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7dQywr2PCPTj"
   },
   "source": [
    "# 1. Preparation\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10287,
     "status": "ok",
     "timestamp": 1656687929737,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "gFK8pPQpCPTk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from exercise_code.data.facial_keypoints_dataset import FacialKeypointsDataset\n",
    "from exercise_code.networks.keypoint_nn import (\n",
    "    DummyKeypointModel,\n",
    "    KeypointModel\n",
    ")\n",
    "from exercise_code.util import (\n",
    "    show_all_keypoints,\n",
    "    save_model,\n",
    ")\n",
    "from exercise_code.tests import test_keypoint_nn\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PzzrR3TuCPTk"
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note: Google Colab</h3>\n",
    "    <p>\n",
    "In case you don't have a GPU, you can run this notebook on Google Colab where you can access a GPU for free, but you can also run this notebook on your CPU.\n",
    "         </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 352,
     "status": "ok",
     "timestamp": 1656687930078,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "4IeJlGMsCPTl",
    "outputId": "414917b6-6003-4960-c6c5-4e633b5ce420"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "m6clRitvCPTm"
   },
   "source": [
    "## Load and Visualize Data\n",
    "To load the data, we have already prepared a Pytorch Dataset class `FacialKeypointsDataset` for you. You can find it in `exercise_code/data/facial_keypoints_dataset.py`. Run the following cell to download the data and initialize your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7291,
     "status": "ok",
     "timestamp": 1656687937366,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "JURilyFQCPTm",
    "outputId": "7d1aa7c9-174c-4bc2-a785-4421658e35b2"
   },
   "outputs": [],
   "source": [
    "download_url = \"https://vision.in.tum.de/webshare/g/i2dl/facial_keypoints.zip\"\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "data_root = os.path.join(i2dl_exercises_path, \"datasets\", \"facial_keypoints\")\n",
    "train_dataset = FacialKeypointsDataset(\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),\n",
    "    root=data_root,\n",
    "    download_url=download_url,\n",
    ")\n",
    "val_dataset = FacialKeypointsDataset(\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor(),\n",
    "    root=data_root,\n",
    ")\n",
    "\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of validation samples:\", len(val_dataset))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zHlKmzEjCPTn"
   },
   "source": [
    "Each sample in our dataset is a dict `{\"image\": image, \"keypoints\": keypoints}`, where\n",
    " * `image` is a [0-1]-normalized gray-scale image of size 96x96, represented by a torch tensor of shape (CxHxW) with C=1, H=96, W=96\n",
    "    <img style=\"float: right;\" src='images/key_pts_expl.png' width=50% height=50%/>\n",
    " * `keypoints` is the list of K facial keypoints, stored in a torch tensor of shape (Kx2). We have K=15 keypoints that stand for:\n",
    "   * keypoints[0]: Center of the left eye\n",
    "   * keypoints[1]: Center of the right eye\n",
    "   * keypoints[2]: Left eye inner corner\n",
    "   * keypoints[3]: Left eye outer corner\n",
    "   * keypoints[4]: Right eye inner corner\n",
    "   * keypoints[5]: Right eye outer corner\n",
    "   * keypoints[6]: Left eyebrow inner end\n",
    "   * keypoints[7]: Left eyebrow outer end\n",
    "   * keypoints[8]: Right eyebrow inner end\n",
    "   * keypoints[9]: Right eyebrow outer end\n",
    "   * keypoints[10]: Nose tip\n",
    "   * keypoints[11]: Mouth left corner\n",
    "   * keypoints[12]: Mouth right corner\n",
    "   * keypoints[13]: Mouth center top lip\n",
    "   * keypoints[14]: Mouth center bottom lip\n",
    "   \n",
    "Each individual facial keypoint is represented by two coordinates (x,y) that specify the horizontal and vertical location of the keypoint respectively. All keypoint values are normalized to be in the range [-1,1], such that:\n",
    "   * (x=-1,y=-1) corresponds to the top left corner, \n",
    "   * (x=-1,y=1) to the bottom left corner,\n",
    "   * (x=1,y=-1) to the top right corner,\n",
    "   * (x=1,y=1) to the bottom right corner,\n",
    "   * and (x=0,y=0) to the center of the image.\n",
    "   \n",
    "      \n",
    "The data downloaded is already preprocessed and hence there is no need to apply transformations in order to prepare the data. Of course, feel free to apply training transformations to improve your performance such as e.g. flipping the training images. </br>\n",
    "\n",
    "**Note**: The data downloaded is already preprocessed and hence there is **no need** to apply normalization transformations in order to prepare the data. Of course, feel free to apply training transformations to improve your performance such as e.g. flipping the training images. </br>\n",
    "\n",
    "Also, when applying transformations such as flipping, make sure that the predicted coordinates of your keypoints change accordingly.\n",
    "\n",
    "Let's have a look at the first training sample to get a better feeling for the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 462,
     "status": "ok",
     "timestamp": 1656687937818,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "-Uv8nLvcCPTn",
    "outputId": "51159487-0682-435f-b9c8-5135b69c1bb0"
   },
   "outputs": [],
   "source": [
    "image, keypoints = train_dataset[0][\"image\"], train_dataset[0][\"keypoints\"]\n",
    "print(\"Shape of the image:\", image.size())\n",
    "print(\"Smallest value in the image:\", torch.min(image))\n",
    "print(\"Largest value in the image:\", torch.max(image))\n",
    "print(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1656687937818,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "D28hhzk-CPTn",
    "outputId": "b4d05fa5-22f0-40c3-fb36-79ad18db05c0"
   },
   "outputs": [],
   "source": [
    "print(keypoints)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XLYOhkx7CPTo"
   },
   "source": [
    "In `exercise_code/util/vis_utils.py` we also provide you with a function `show_all_keypoints()` that takes in an image and keypoints and displays where the predicted keypoints are in the image. Let's use it to plot the first few images of our training set:\n",
    "\n",
    "**Note:** if your kernel dies when running the following cell, please uncomment the last line of the imports cell `os.environ['KMP_DUPLICATE_LIB_OK']='True'`and try it again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1536,
     "status": "ok",
     "timestamp": 1656687939350,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "VQRWKbRCCPTo",
    "outputId": "02626bce-2b0f-477a-9006-074a9b073b6b"
   },
   "outputs": [],
   "source": [
    "def show_keypoints(dataset, num_samples=3):\n",
    "    for i in range(num_samples):\n",
    "        image = dataset[i][\"image\"]\n",
    "        key_pts = dataset[i][\"keypoints\"]\n",
    "        show_all_keypoints(image, key_pts)\n",
    "\n",
    "\n",
    "show_keypoints(train_dataset)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "mZap99KcCPTo"
   },
   "source": [
    "# 2. Facial Keypoint Detection\n",
    "Your task is to define and train a model for facial keypoint detection.\n",
    "\n",
    "The facial keypoint detection task can be seen as a regression problem, where the goal is to predict 30 different values that correspond to the 15 facial keypoint locations. Thus, we need to build a network that gets a (1x96x96) image as input and predicts 30 continuous outputs between [-1,1].\n",
    "\n",
    "## Dummy Model\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>In <code>exercise_code/networks/keypoint_nn.py</code> we defined a naive <code>DummyKeypointModel</code>, which always predicts the keypoints of the first training image in the dataset. Let's try it on a few images and visualize our predictions in red:\n",
    " </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1656687939580,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "aQLwlDXjCPTo"
   },
   "outputs": [],
   "source": [
    "def show_keypoint_predictions(model, dataset, num_samples=3):\n",
    "    for i in range(num_samples):\n",
    "        image = dataset[i][\"image\"].to(device)\n",
    "        key_pts = dataset[i][\"keypoints\"].to(device)\n",
    "        predicted_keypoints = torch.squeeze(model(image).detach()).view(15, 2)\n",
    "        show_all_keypoints(image, key_pts, predicted_keypoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1105,
     "status": "ok",
     "timestamp": 1656687940681,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "IbbBJRmhCPTp",
    "outputId": "bd507cf0-c8b7-4dff-b65a-bfdb6c2e2ad7"
   },
   "outputs": [],
   "source": [
    "dummy_model = DummyKeypointModel()\n",
    "show_keypoint_predictions(dummy_model, train_dataset)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eVKNbtcTCPTp"
   },
   "source": [
    "As we see, the model predicts the first sample perfectly, but for the remaining samples the predictions are quite off.\n",
    "\n",
    "## Loss and Metrics\n",
    "\n",
    "To measure the quality of the model's predictions, we will use the mean squared error (https://en.wikipedia.org/wiki/Mean_squared_error), summed up over all 30 keypoint locations. In PyTorch, the mean squared error is defined in `torch.nn.MSELoss()`, and we can use it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1656687941151,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "CeoyyehrCPTp",
    "outputId": "81b6ef9e-80aa-4a3c-8d62-8570fddb64ff"
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "for i in range(3):\n",
    "    image = train_dataset[i][\"image\"]\n",
    "    keypoints = train_dataset[i][\"keypoints\"]\n",
    "    predicted_keypoints = torch.squeeze(dummy_model(image)).view(15, 2)\n",
    "    loss = loss_fn(keypoints, predicted_keypoints)\n",
    "    print(\"Loss on image %d:\" % i, loss)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KrYEcXddCPTp"
   },
   "source": [
    "As expected, our dummy model achieves a loss close to 0 on the first sample, but on all other samples the loss is quite high.\n",
    "\n",
    "To obtain an evaluation score (in the notebook and on the submission server), we will use the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1208,
     "status": "ok",
     "timestamp": 1656687942353,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "zC13aydNCPTq",
    "outputId": "016dca5b-43ae-4089-a390-0ce3b7c838b2"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss = 0\n",
    "    for batch in dataloader:\n",
    "        image, keypoints = batch[\"image\"].to(device), batch[\"keypoints\"].to(device)\n",
    "        predicted_keypoints = model(image).view(-1, 15, 2).to(device)\n",
    "        loss += criterion(\n",
    "            torch.squeeze(keypoints), torch.squeeze(predicted_keypoints)\n",
    "        ).item()\n",
    "    return 1.0 / (2 * (loss / len(dataloader)))\n",
    "\n",
    "\n",
    "print(\"Score of the Dummy Model:\", evaluate_model(dummy_model, val_dataset))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lzbbMeE4CPTq"
   },
   "source": [
    "**To pass the assignment, you will need to achieve a score of at least 100**. As you can see, the score is calculated from the average loss, so **your average loss needs to be lower than 0.005**. Our dummy model only gets a score of around 60, so you will have to come up with a better model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4GCE6hqECPTr"
   },
   "source": [
    "## Step 1: Design your own model\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p> Now it is your turn to build your own model. To do so, you need to design a convolution neural network that takes images of size (Nx1x96x96) as input and produces outputs of shape (Nx30) in the range [-1,1]. Therefore, implement the <code>KeypointModel</code> class in <code>exercise_code/networks/keypoint_nn.py</code>.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "Recall that CNN's are defined by a few types of layers:\n",
    "* Convolutional layers\n",
    "* Max-pooling layers\n",
    "* Fully-connected layers\n",
    "\n",
    "You can design your network however you want, but we strongly suggest to include multiple convolution layers. You are also encouraged to use things like dropout and batch normalization to stabilize and regularize your network. If you want to build a really competitive model, have a look at some literature on keypoint detection, such as [this paper](https://arxiv.org/pdf/1710.00977.pdf).\n",
    "\n",
    "#### Define your model in the provided file \n",
    "`exercise_code/networks/keypoint_nn.py` file\n",
    "\n",
    "This file is mostly empty but contains the expected class name, and the methods that your model needs to implement (only `forward()` basically).\n",
    "\n",
    "The only rules your model design has to follow are:\n",
    "* Perform the forward pass in forward(), predicting keypoints of shape (Nx30) for images of shape (Nx1x96x96)\n",
    "* Have less than 5 million parameters\n",
    "* Have a model size of less than 20MB after saving\n",
    "\n",
    "Furthermore, you need to pass all your hyperparameters to the model in a single dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1656687942354,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "9tw7h1YtCPTr"
   },
   "outputs": [],
   "source": [
    "\n",
    "hparams = {\n",
    "    \"device\" : device,\n",
    "    \"num_workers\" : 8,\n",
    "    \"epochs\" : 50,\n",
    "    \"batch_size\" : 64, # TODO: Change to 128 or 256 when doing final big model, but batch size slows learning.\n",
    "    \n",
    "    \"max_patience\" : 10, # Stop early if overfitting, although you never really know\n",
    "    \n",
    "    \"input_size\" : 96, # Greyscale image is 96 * 96 ()\n",
    "    \"output_size\" : 30, # 15 (x,y) keypoints flattened to vector = 30\n",
    "    \n",
    "    # Because we have a limited num of parameters, we will need to maxpool the input already\n",
    "    # Tbh with the additional maxpools kernel size of 1 is fine\n",
    "    'input_pool_kernel' : 1,\n",
    "    'input_pool_padding' : 0,\n",
    "    \n",
    "    # Theory: if we have low number of kernels, then we need high % to be able to find features. Thus a high dropout\n",
    "    # will result in poor generalization, it simply will not be able to find features well because it does\n",
    "    # not have access to enough kernels. So more kernels if you want higher dropout, but since this results in\n",
    "    # more weights in front of linear layer, also need more convolutional layers to keep model under 5 million.\n",
    "    \n",
    "    # Conv Layer 1\n",
    "    \"conv1_out_channels\" : 16 * 4, # Num of output channels\n",
    "    \"conv1_kernel\" : 5, # Kernel size\n",
    "    \"conv1_padding\" : 0,\n",
    "    \"conv1_stride\" : 1,\n",
    "    \"conv1_pooling_kernel\" : 2, # Maxpool kernel\n",
    "    # \"conv1_dropout\" : 0.0, # Only use dropout once happy with basic model. And not too large, massively slows convergence\n",
    "    \n",
    "    # Conv Layer 2\n",
    "    \"conv2_out_channels\" : 16 * 8, # Num of output channels\n",
    "    \"conv2_kernel\" : 3, # Kernel size\n",
    "    \"conv2_padding\" : 0,\n",
    "    \"conv2_stride\" : 1,\n",
    "    \"conv2_pooling_kernel\" : 2,\n",
    "    # \"conv2_dropout\" : 0.0,\n",
    "     \n",
    "    # Conv Layer 3\n",
    "    \"conv3_out_channels\" : 16 * 16, # Num of output channels\n",
    "    \"conv3_kernel\" : 2, # Kernel size\n",
    "    \"conv3_padding\" : 0,\n",
    "    \"conv3_stride\" : 1,\n",
    "    \"conv3_pooling_kernel\" : 2,\n",
    "    # \"conv3_dropout\" : 0.0,\n",
    "    \n",
    "    # Conv Layer 4\n",
    "    # \"conv4_out_channels\" : 16 * 16, # Num of output channels\n",
    "    # \"conv4_kernel\" : 2, # Kernel size\n",
    "    # \"conv4_padding\" : 0,\n",
    "    # \"conv4_stride\" : 1,\n",
    "    # \"conv4_pooling_kernel\" : 2,\n",
    "    # \"conv4_dropout\" : 0.0, # Use batchnorm or dropout, not both\n",
    "    \n",
    "    # Linear layers\n",
    "    \"linear_weights\" : 180,\n",
    "    'linear_dropout' : 0.0,\n",
    "    \n",
    "    \"learning_rate\" : 10e-4, # Optimizer\n",
    "    \"weight_decay\" : 1e-8, # ADAM, use to reduce big fluctuations in loss graphs\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GtMden_jCPTr"
   },
   "source": [
    "To test whether your model follows the basic rules, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 585,
     "status": "ok",
     "timestamp": 1656687942936,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "zkFiXrhOCPTs",
    "outputId": "add2801f-fb10-49cf-b5c0-28c59dfb3c8c"
   },
   "outputs": [],
   "source": [
    "model = KeypointModel(hparams)\n",
    "test_keypoint_nn(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ja1w927OCPTs"
   },
   "source": [
    "## Step 2: Train your model\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p> In addition to the network itself, you will also need to write the code for the model training.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "**Hints:**\n",
    "* Use `torch.nn.MSELoss()` as loss function.\n",
    "\n",
    "* You have two options for training code:\n",
    "    - Use a straightforward training scheme. See 1.pytorch.ipynb from ex07.\n",
    "    - Don't call your model anything else besides \"model\", unless you notice that you'll need to modify the model name in the upcoming cells.\n",
    "    - You could refer to the training scheme from exercise_08 to complete your code. Understanding this pipeline is crucial for future works in deep learning.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timm helper code\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable),total=len(iterable), ncols=150, desc=desc)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_func, tb_logger, epochs=3, name='keypoints'):\n",
    "    \n",
    "    # Use for LR scheduling\n",
    "    optimizer = model.optimizer\n",
    "    # TODO: Set gamma or factor as hparam, as well as epochs patience\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=epochs * len(train_loader) / 5, gamma=0.7)\n",
    "    \n",
    "    # Use this to try and get really small losses. patience is set so that LR updated only every epoch\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=len(train_loader)) \n",
    "    validation_loss = 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Used to end training early if we are starting to overfit\n",
    "    patience = model.hparams[\"max_patience\"]\n",
    "    best_val_loss = 10e10 # Best val loss of an epoch. Init as unrealistically large\n",
    "    \n",
    "    for epoch in range(epochs):        \n",
    "        \n",
    "        # Training\n",
    "        training_loop = create_tqdm_bar(train_loader, desc=f'Training Epoch [{epoch + 1}/{epochs}]')\n",
    "        training_loss = 0\n",
    "        for train_iteration, batch in training_loop:\n",
    "            loss = model.training_step(batch, loss_func)\n",
    "            training_loss += loss.item()\n",
    "            scheduler.step(best_val_loss) # TODO: Is this right place to do scheduler step?\n",
    "            \n",
    "            # Update the progress bar\n",
    "            training_loop.set_postfix(train_loss = \"{:.8f}\".format(training_loss / (train_iteration + 1)), lr = \"{:.8f}\".format(optimizer.param_groups[0]['lr']))\n",
    "            \n",
    "            # Update the tensorboard logger.\n",
    "            tb_logger.add_scalar(f'{name}/train_loss', loss.item(), epoch * len(train_loader) + train_iteration)\n",
    "            \n",
    "        # Validation\n",
    "        val_loop = create_tqdm_bar(val_loader, desc=f'Validation Epoch [{epoch + 1}/{epochs}]')\n",
    "        validation_loss = 0\n",
    "        with torch.no_grad(): # Don't actually need because validation_step already has it?\n",
    "            for val_iteration, batch in val_loop:\n",
    "                loss = model.validation_step(batch, loss_func)\n",
    "                validation_loss += loss.item()\n",
    "                \n",
    "                # Update the progress bar\n",
    "                val_loop.set_postfix(patience =\"{}\".format(patience), val_loss = \"{:.8f}\".format(patience, validation_loss / (val_iteration + 1)))\n",
    "                \n",
    "                # Update the tensorboard logger.\n",
    "                tb_logger.add_scalar(f'{name}/val_loss', validation_loss / (val_iteration + 1), epoch * len(val_loader) + val_iteration)\n",
    "        \n",
    "        scaled_loss =  validation_loss / (val_iteration + 1)# validation_loss if sum of iteration, we want average\n",
    "        if scaled_loss <= best_val_loss :\n",
    "            patience = model.hparams[\"max_patience\"]\n",
    "            best_val_loss = scaled_loss # Rescaled based on batch\n",
    "        else:\n",
    "            patience -= 1\n",
    "            \n",
    "        # End program if patience is done\n",
    "        if patience == 0:\n",
    "            print(f'Stopping early at epoch {epoch}! (patience done)')\n",
    "            print(f'Best Val loss: {best_val_loss}. Final Val loss used: {scaled_loss}' )\n",
    "            break\n",
    "        \n",
    "        # also end program early if we get a big overshoot, want to get a really small val loss\n",
    "        # uses validation_loss from the previous loop\n",
    "        if scaled_loss <= 6e-4:\n",
    "            print(f'Stopping early at epoch {epoch}! (Found low value)')\n",
    "            print(f'Best Val loss: {best_val_loss}. Final Val loss used: {scaled_loss}' )\n",
    "            break\n",
    "        \n",
    "        \n",
    "        \n",
    "        # This value is for the progress bar of the training loop.\n",
    "        validation_loss /= len(val_loader)\n",
    "        \n",
    "    print(f'Best Val loss: {best_val_loss}. Final Val loss used: {scaled_loss}' )\n",
    "        \n",
    "        \n",
    "        \n",
    "# Helper function to visualize the convolution layers (DOESNT WORK)\n",
    "# def visualize_conv(model):\n",
    "    \n",
    "#     counter = 0 # Number of Convolution layers\n",
    "    \n",
    "#     for layer in model.model:\n",
    "#         if type(layer) == nn.Conv2d:\n",
    "#             counter += 1\n",
    "#             print(layer.weight.data.shape)\n",
    "\n",
    "#             num_channels = layer.weight.data.shape[0] # Number of kernels\n",
    "#             ker_x, ker_y = layer.weight.data.shape[2], layer.weight.data.shape[3] # Dimensions of kernel\n",
    "#             num_cols = 6\n",
    "#             num_rows = 1 + num_channels // num_cols\n",
    "            \n",
    "#             fig = plt.figure(figsize=(num_cols,num_rows))\n",
    "            \n",
    "#             for i in range(num_channels):\n",
    "#                 ax1 = fig.add_subplot(num_rows,num_cols,i+1)\n",
    "#                 ax1.imshow(layer.weight.data[i].reshape((ker_x, ker_y)))\n",
    "#                 ax1.axis('off')\n",
    "#                 ax1.set_xticklabels([])\n",
    "#                 ax1.set_yticklabels([])\n",
    "        \n",
    "#         # Show image for every layer        \n",
    "#         plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "#         plt.show()\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "#     print(f'Number of Convolution layers: {counter}')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 100251,
     "status": "ok",
     "timestamp": 1656688055600,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "7fRJmnLnCPTs",
    "outputId": "87a13392-7be9-4296-e12b-d2d8c3878578"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/50]: 100%|██████████████████████████████| 25/25 [00:16<00:00,  1.53it/s, train_loss=0.06844889, val_loss=0.00000000 (LR:0.00100000)]\n",
      "Validation Epoch [1/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.17it/s, val_loss=(P10)0.00982764]\n",
      "Training Epoch [2/50]: 100%|██████████████████████████████| 25/25 [00:17<00:00,  1.47it/s, train_loss=0.00888297, val_loss=0.00982764 (LR:0.00100000)]\n",
      "Validation Epoch [2/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.13it/s, val_loss=(P10)0.00440364]\n",
      "Training Epoch [3/50]: 100%|██████████████████████████████| 25/25 [00:16<00:00,  1.48it/s, train_loss=0.00482489, val_loss=0.00440364 (LR:0.00100000)]\n",
      "Validation Epoch [3/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.21it/s, val_loss=(P10)0.00285251]\n",
      "Training Epoch [4/50]: 100%|██████████████████████████████| 25/25 [00:16<00:00,  1.55it/s, train_loss=0.00370695, val_loss=0.00285251 (LR:0.00100000)]\n",
      "Validation Epoch [4/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.29it/s, val_loss=(P10)0.00224615]\n",
      "Training Epoch [5/50]: 100%|██████████████████████████████| 25/25 [00:16<00:00,  1.52it/s, train_loss=0.00322087, val_loss=0.00224615 (LR:0.00100000)]\n",
      "Validation Epoch [5/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.34it/s, val_loss=(P10)0.00224019]\n",
      "Training Epoch [6/50]: 100%|██████████████████████████████| 25/25 [00:16<00:00,  1.54it/s, train_loss=0.00288914, val_loss=0.00224019 (LR:0.00100000)]\n",
      "Validation Epoch [6/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.15it/s, val_loss=(P10)0.00239186]\n",
      "Training Epoch [7/50]: 100%|██████████████████████████████| 25/25 [00:16<00:00,  1.52it/s, train_loss=0.00262005, val_loss=0.00239186 (LR:0.00070000)]\n",
      "Validation Epoch [7/50]: 100%|█████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.28it/s, val_loss=(P9)0.00160541]\n",
      "Training Epoch [8/50]: 100%|██████████████████████████████| 25/25 [00:16<00:00,  1.53it/s, train_loss=0.00239412, val_loss=0.00160541 (LR:0.00070000)]\n",
      "Validation Epoch [8/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.17it/s, val_loss=(P10)0.00130636]\n",
      "Training Epoch [9/50]: 100%|██████████████████████████████| 25/25 [00:16<00:00,  1.55it/s, train_loss=0.00189686, val_loss=0.00130636 (LR:0.00070000)]\n",
      "Validation Epoch [9/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.23it/s, val_loss=(P10)0.00132153]\n",
      "Training Epoch [10/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.51it/s, train_loss=0.00185968, val_loss=0.00132153 (LR:0.00049000)]\n",
      "Validation Epoch [10/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.19it/s, val_loss=(P9)0.00137568]\n",
      "Training Epoch [11/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.51it/s, train_loss=0.00191139, val_loss=0.00137568 (LR:0.00034300)]\n",
      "Validation Epoch [11/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.13it/s, val_loss=(P8)0.00110885]\n",
      "Training Epoch [12/50]: 100%|█████████████████████████████| 25/25 [00:17<00:00,  1.46it/s, train_loss=0.00146069, val_loss=0.00110885 (LR:0.00034300)]\n",
      "Validation Epoch [12/50]: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.38it/s, val_loss=(P10)0.00108938]\n",
      "Training Epoch [13/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.54it/s, train_loss=0.00161909, val_loss=0.00108938 (LR:0.00034300)]\n",
      "Validation Epoch [13/50]: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.33it/s, val_loss=(P10)0.00105855]\n",
      "Training Epoch [14/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.48it/s, train_loss=0.00148409, val_loss=0.00105855 (LR:0.00034300)]\n",
      "Validation Epoch [14/50]: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.32it/s, val_loss=(P10)0.00105789]\n",
      "Training Epoch [15/50]: 100%|█████████████████████████████| 25/25 [00:15<00:00,  1.57it/s, train_loss=0.00148030, val_loss=0.00105789 (LR:0.00034300)]\n",
      "Validation Epoch [15/50]: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.22it/s, val_loss=(P10)0.00094780]\n",
      "Training Epoch [16/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.55it/s, train_loss=0.00147775, val_loss=0.00094780 (LR:0.00034300)]\n",
      "Validation Epoch [16/50]: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.28it/s, val_loss=(P10)0.00109590]\n",
      "Training Epoch [17/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.55it/s, train_loss=0.00125718, val_loss=0.00109590 (LR:0.00024010)]\n",
      "Validation Epoch [17/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.29it/s, val_loss=(P9)0.00090364]\n",
      "Training Epoch [18/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.53it/s, train_loss=0.00131238, val_loss=0.00090364 (LR:0.00024010)]\n",
      "Validation Epoch [18/50]: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.37it/s, val_loss=(P10)0.00086431]\n",
      "Training Epoch [19/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.51it/s, train_loss=0.00137364, val_loss=0.00086431 (LR:0.00024010)]\n",
      "Validation Epoch [19/50]: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.16it/s, val_loss=(P10)0.00090656]\n",
      "Training Epoch [20/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.50it/s, train_loss=0.00112871, val_loss=0.00090656 (LR:0.00016807)]\n",
      "Validation Epoch [20/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.15it/s, val_loss=(P9)0.00082445]\n",
      "Training Epoch [21/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.50it/s, train_loss=0.00115671, val_loss=0.00082445 (LR:0.00016807)]\n",
      "Validation Epoch [21/50]: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.28it/s, val_loss=(P10)0.00079906]\n",
      "Training Epoch [22/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.51it/s, train_loss=0.00111176, val_loss=0.00079906 (LR:0.00016807)]\n",
      "Validation Epoch [22/50]: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.07it/s, val_loss=(P10)0.00082534]\n",
      "Training Epoch [23/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.52it/s, train_loss=0.00107191, val_loss=0.00082534 (LR:0.00011765)]\n",
      "Validation Epoch [23/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.33it/s, val_loss=(P9)0.00077877]\n",
      "Training Epoch [24/50]: 100%|█████████████████████████████| 25/25 [00:17<00:00,  1.45it/s, train_loss=0.00106533, val_loss=0.00077877 (LR:0.00011765)]\n",
      "Validation Epoch [24/50]: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.03it/s, val_loss=(P10)0.00080277]\n",
      "Training Epoch [25/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.52it/s, train_loss=0.00103991, val_loss=0.00080277 (LR:0.00008235)]\n",
      "Validation Epoch [25/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.39it/s, val_loss=(P9)0.00073595]\n",
      "Training Epoch [26/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.52it/s, train_loss=0.00104672, val_loss=0.00073595 (LR:0.00008235)]\n",
      "Validation Epoch [26/50]: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.23it/s, val_loss=(P10)0.00073283]\n",
      "Training Epoch [27/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.48it/s, train_loss=0.00095585, val_loss=0.00073283 (LR:0.00008235)]\n",
      "Validation Epoch [27/50]: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.28it/s, val_loss=(P10)0.00073111]\n",
      "Training Epoch [28/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.51it/s, train_loss=0.00099137, val_loss=0.00073111 (LR:0.00008235)]\n",
      "Validation Epoch [28/50]: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.27it/s, val_loss=(P10)0.00076041]\n",
      "Training Epoch [29/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.52it/s, train_loss=0.00091017, val_loss=0.00076041 (LR:0.00005765)]\n",
      "Validation Epoch [29/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.40it/s, val_loss=(P9)0.00072653]\n",
      "Training Epoch [30/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.54it/s, train_loss=0.00104123, val_loss=0.00072653 (LR:0.00005765)]\n",
      "Validation Epoch [30/50]: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.45it/s, val_loss=(P10)0.00074007]\n",
      "Training Epoch [31/50]: 100%|█████████████████████████████| 25/25 [00:15<00:00,  1.58it/s, train_loss=0.00089515, val_loss=0.00074007 (LR:0.00004035)]\n",
      "Validation Epoch [31/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.06it/s, val_loss=(P9)0.00073117]\n",
      "Training Epoch [32/50]: 100%|█████████████████████████████| 25/25 [00:17<00:00,  1.47it/s, train_loss=0.00091024, val_loss=0.00073117 (LR:0.00002825)]\n",
      "Validation Epoch [32/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.09it/s, val_loss=(P8)0.00072562]\n",
      "Training Epoch [33/50]: 100%|█████████████████████████████| 25/25 [00:15<00:00,  1.57it/s, train_loss=0.00102409, val_loss=0.00072562 (LR:0.00002825)]\n",
      "Validation Epoch [33/50]: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.30it/s, val_loss=(P10)0.00075356]\n",
      "Training Epoch [34/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.50it/s, train_loss=0.00094372, val_loss=0.00075356 (LR:0.00001977)]\n",
      "Validation Epoch [34/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.25it/s, val_loss=(P9)0.00073741]\n",
      "Training Epoch [35/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.50it/s, train_loss=0.00098592, val_loss=0.00073741 (LR:0.00001384)]\n",
      "Validation Epoch [35/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.09it/s, val_loss=(P8)0.00071459]\n",
      "Training Epoch [36/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.50it/s, train_loss=0.00091489, val_loss=0.00071459 (LR:0.00001384)]\n",
      "Validation Epoch [36/50]: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.20it/s, val_loss=(P10)0.00071037]\n",
      "Training Epoch [37/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.53it/s, train_loss=0.00090907, val_loss=0.00071037 (LR:0.00001384)]\n",
      "Validation Epoch [37/50]: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.20it/s, val_loss=(P10)0.00070648]\n",
      "Training Epoch [38/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.54it/s, train_loss=0.00093571, val_loss=0.00070648 (LR:0.00001384)]\n",
      "Validation Epoch [38/50]: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.29it/s, val_loss=(P10)0.00072042]\n",
      "Training Epoch [39/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.50it/s, train_loss=0.00081673, val_loss=0.00072042 (LR:0.00000969)]\n",
      "Validation Epoch [39/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.15it/s, val_loss=(P9)0.00071848]\n",
      "Training Epoch [40/50]: 100%|█████████████████████████████| 25/25 [00:17<00:00,  1.47it/s, train_loss=0.00081228, val_loss=0.00071848 (LR:0.00000678)]\n",
      "Validation Epoch [40/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.37it/s, val_loss=(P8)0.00072468]\n",
      "Training Epoch [41/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.55it/s, train_loss=0.00082379, val_loss=0.00072468 (LR:0.00000475)]\n",
      "Validation Epoch [41/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.17it/s, val_loss=(P7)0.00070083]\n",
      "Training Epoch [42/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.48it/s, train_loss=0.00085167, val_loss=0.00070083 (LR:0.00000475)]\n",
      "Validation Epoch [42/50]: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.16it/s, val_loss=(P10)0.00070850]\n",
      "Training Epoch [43/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.50it/s, train_loss=0.00081876, val_loss=0.00070850 (LR:0.00000332)]\n",
      "Validation Epoch [43/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.22it/s, val_loss=(P9)0.00076398]\n",
      "Training Epoch [44/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.52it/s, train_loss=0.00081287, val_loss=0.00076398 (LR:0.00000233)]\n",
      "Validation Epoch [44/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.25it/s, val_loss=(P8)0.00071896]\n",
      "Training Epoch [45/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.56it/s, train_loss=0.00087710, val_loss=0.00071896 (LR:0.00000163)]\n",
      "Validation Epoch [45/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.27it/s, val_loss=(P7)0.00073393]\n",
      "Training Epoch [46/50]: 100%|█████████████████████████████| 25/25 [00:17<00:00,  1.45it/s, train_loss=0.00080151, val_loss=0.00073393 (LR:0.00000114)]\n",
      "Validation Epoch [46/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.15it/s, val_loss=(P6)0.00072031]\n",
      "Training Epoch [47/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.53it/s, train_loss=0.00082149, val_loss=0.00072031 (LR:0.00000080)]\n",
      "Validation Epoch [47/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.21it/s, val_loss=(P5)0.00073160]\n",
      "Training Epoch [48/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.51it/s, train_loss=0.00089797, val_loss=0.00073160 (LR:0.00000056)]\n",
      "Validation Epoch [48/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.19it/s, val_loss=(P4)0.00077800]\n",
      "Training Epoch [49/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.53it/s, train_loss=0.00092311, val_loss=0.00077800 (LR:0.00000039)]\n",
      "Validation Epoch [49/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.26it/s, val_loss=(P3)0.00074315]\n",
      "Training Epoch [50/50]: 100%|█████████████████████████████| 25/25 [00:16<00:00,  1.48it/s, train_loss=0.00085148, val_loss=0.00074315 (LR:0.00000027)]\n",
      "Validation Epoch [50/50]: 100%|████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.04it/s, val_loss=(P2)0.00071361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Val loss: 0.000700831797439605. Final Val loss used: 0.0007136135012842715\n",
      "Score of the Model: 694.5646579482235\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# TODO - Train Your Model                                              #\n",
    "########################################################################\n",
    "\n",
    "model = KeypointModel(hparams)\n",
    "\n",
    "# Create the tb_logger\n",
    "path = os.path.join('logs', 'keypoint_logs')\n",
    "num_of_runs = len(os.listdir(path)) if os.path.exists(path) else 0\n",
    "path = os.path.join(path, f'run_{num_of_runs + 1}')\n",
    "tb_logger = SummaryWriter(path)\n",
    "\n",
    "# Train the model\n",
    "# TODO: Maybe try to use a small dataset first?\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=hparams['batch_size'], shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=hparams['batch_size'], shuffle=False)\n",
    "\n",
    "epochs = hparams.get('epochs', 5)\n",
    "loss_func = nn.MSELoss()\n",
    "train_model(model, train_loader, val_loader, loss_func, tb_logger, epochs=epochs, name='keypoints')\n",
    "\n",
    "# We want a score of at least 100 here\n",
    "print(\"Score of the Model:\", evaluate_model(model, val_dataset))\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RuNDv7kFCPTs"
   },
   "source": [
    "When you're done training, run the cells below to visualize some predictions of your model, and to compute a validation score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1124,
     "status": "ok",
     "timestamp": 1656688135189,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "W7-SOD-CCPTt",
    "outputId": "88844c59-737a-4139-9de6-c3e54cb994f5"
   },
   "outputs": [],
   "source": [
    "show_keypoint_predictions(model, val_dataset, num_samples=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5848,
     "status": "ok",
     "timestamp": 1656688141265,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "ksQY1k0iCPTt",
    "outputId": "1e96371b-d07b-4076-912f-321743f4dc91"
   },
   "outputs": [],
   "source": [
    "print(\"Score:\", evaluate_model(model, val_dataset))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "fvOocX1FCPTt"
   },
   "source": [
    "# 3. Save Your Model for Submission\n",
    "\n",
    "If your model achieved a validation score of 100 or higher, save your model with the cell below and submit it to [the submission server](https://i2dl.vc.in.tum.de/). Your validation set is of course different from the test set on our server, so results may vary. Nevertheless, you will have a reasonable close approximation about your performance.\n",
    "\n",
    "Before that, we will check again whether the number of parameters is below 5 Mio. and the file size is below 20 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1656688141613,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "Df44Fc0zCPTt",
    "outputId": "3bd1c1ba-d750-436a-bf2c-998837b30ac6"
   },
   "outputs": [],
   "source": [
    "save_model(model, \"facial_keypoints.p\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "fvcX9_h5CPTt"
   },
   "source": [
    "Congrats - you've now finished your first Convolution Neural Network! Simply run the following cell to create a zipped file for your implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8749,
     "status": "ok",
     "timestamp": 1656688150356,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "w_QV7M1uCPTt",
    "outputId": "77b2de33-dbf0-4fe8-fedc-bc4ec7fd9bb0"
   },
   "outputs": [],
   "source": [
    "# Now zip the folder for upload\n",
    "from exercise_code.util.submit import submit_exercise\n",
    "\n",
    "submit_exercise(\"../output/exercise09\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Jrl8SOl5CPTt"
   },
   "source": [
    "# Submission Goals\n",
    "\n",
    "- Goal: Implement and train a convolution neural network for facial keypoint detection.\n",
    "- Passing Criteria: Reach **Score >= 100** on __our__ test dataset. The submission system will show you your score after you submit.\n",
    "\n",
    "- Submission start: __June 22, 2023, 10.00__\n",
    "- Submission deadline: __June 28, 2023 - 15:59__ \n",
    "- You can make **$\\infty$** submissions until the deadline. Your __best submission__ will be considered for bonus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Exercise Review](https://docs.google.com/forms/d/e/1FAIpQLSedSHEJ5vN-9FmJN-IGSQ9heDM_8qJQjHL4glgQGlrpQJEYPQ/viewform?usp=pp_url&entry.999074405=Exercise+9:+Facial+Keypoints)\n",
    "We are always interested in your opinion. Now that you have finished this exercise, we would like you to give us some feedback about the time required to finish the submission and/or work through the notebooks. Please take the short time to fill out our [review form](https://docs.google.com/forms/d/e/1FAIpQLSedSHEJ5vN-9FmJN-IGSQ9heDM_8qJQjHL4glgQGlrpQJEYPQ/viewform?usp=pp_url&entry.999074405=Exercise+9:+Facial+Keypoints) for this exercise so that we can do better next time! :)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1_facial_keypoints.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "54970da6898dad277dbf355945c2dee7f942d2a31ec1fc1455b6d4f552d07b83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
